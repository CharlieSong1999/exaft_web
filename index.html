<!DOCTYPE html>
<html lang="en">
<head>
  <script>
    window.MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Extreme amodal face detection - Changlin Song, Yunzhong Hou, Michael Randall Barnes, Rahul Shome, Dylan Campbell">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Detect faces beyond input field-of-view">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Changlin Song, Yunzhong Hou, Michael Randall Barnes, Rahul Shome, Dylan Campbell">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Australian National University">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Extreme amodal face detection">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="Detect faces beyond input field-of-view">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://charliesong1999.github.io/exaft_web/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Extreme amodal face detection - Research Preview">
  <meta property="article:published_time" content="2025-10-08T00:00:00.000Z">
  <meta property="article:author" content="Changlin Song">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Computer Vision">
  <meta property="article:tag" content="Extreme amodal face detection">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="Extreme amodal face detection">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Extreme amodal face detection">
  <meta name="citation_author" content="Changlin, Song">
  <meta name="citation_author" content="Yunzhong, Hou">
  <meta name="citation_author" content="Michael Randall, Barnes">
  <meta name="citation_author" content="Rahul, Shome">
  <meta name="citation_author" content="Dylan, Campbell">
  <meta name="citation_publication_date" content="2025">
  <!-- <meta name="citation_conference_title" content="CONFERENCE_NAME"> -->
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Extreme amodal face detection - Changlin Song, Yunzhong Hou, Michael Randall Barnes, Rahul Shome, Dylan Campbell</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <!-- <div class="more-works-container"> -->
    <!-- <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab"> -->
      <!-- <i class="fas fa-flask"></i> -->
      <!-- More Works -->
      <!-- <i class="fas fa-chevron-down dropdown-arrow"></i> -->
    <!-- </button> -->
    <!-- <div class="more-works-dropdown" id="moreWorksDropdown"> -->
      <!-- <div class="dropdown-header"> -->
        <!-- <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- TODO: Replace with your lab's related works -->
        <!-- <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank"> -->
          <!-- <div class="work-info"> -->
            <!-- TODO: Replace with actual paper title -->
            <!-- <h5>Paper Title 1</h5> -->
            <!-- TODO: Replace with brief description -->
            <!-- <p>Brief description of the work and its main contribution.</p> -->
            <!-- TODO: Replace with venue and year -->
            <!-- <span class="work-venue">Conference/Journal 2024</span> -->
          <!-- </div> -->
          <!-- <i class="fas fa-external-link-alt"></i> -->
        <!-- </a> -->
        <!-- TODO: Add more related works or remove extra items -->
        <!-- <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank"> -->
          <!-- <div class="work-info"> -->
            <!-- <h5>Paper Title 2</h5> -->
            <!-- <p>Brief description of the work and its main contribution.</p> -->
            <!-- <span class="work-venue">Conference/Journal 2023</span> -->
          <!-- </div> -->
          <!-- <i class="fas fa-external-link-alt"></i> -->
        <!-- </a> -->
        <!-- <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank"> -->
          <!-- <div class="work-info"> -->
            <!-- <h5>Paper Title 3</h5> -->
            <!-- <p>Brief description of the work and its main contribution.</p> -->
            <!-- <span class="work-venue">Conference/Journal 2023</span> -->
          <!-- </div> -->
          <!-- <i class="fas fa-external-link-alt"></i> -->
        <!-- </a> -->
      <!-- </div> -->
    <!-- </div> -->
  <!-- </div> -->

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">Extreme Amodal Face Detection</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://charliesong1999.github.io/" target="_blank">Changlin Song</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://hou-yz.github.io/" target="_blank">Yunzhong Hou</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://michaelrandallbarnes.com/about" target="_blank">Michael Randall Barnes</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://rahulsho.me/" target="_blank">Rahul Shome</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://sites.google.com/view/djcampbell" target="_blank">Dylan Campbell</a><sup>1</sup>,
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block"><sup>1</sup>Australian National University</span><span><sup>2</sup>University of Oslo</span>
                    <!-- <span class="author-block"><sup>1</sup>Institution Name<br>Conference name and year</span> -->
                    <!-- TODO: Remove this line if no equal contribution -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2510.06791" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/CharlieSong1999/EXAFace/tree/main" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <span class="link-block">
                      <a href="https://drive.google.com/drive/folders/1FM-YG7vuBazMkVu-3es4PPXXuxigHiuF?dmr=1&ec=wgc-drive-hero-gotof" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-google-drive"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2510.06791" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser"> -->
  <!-- <div class="container is-max-desktop"> -->
    <!-- <div class="hero-body"> -->
      <!-- TODO: Replace with your teaser video -->
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata"> -->
        <!-- TODO: Add your video file path here -->
        <!-- <source src="static/videos/banner_video.mp4" type="video/mp4"> -->
      <!-- </video> -->
      <!-- TODO: Replace with your video description -->
      <!-- <h2 class="subtitle has-text-centered"> -->
        <!-- Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.  -->
      <!-- </h2> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Extreme amodal detection is the task of inferring the 2D location of objects that are not fully visible in the input image but are visible within an expanded field-of-view.
            This differs from amodal detection, where the object is partially visible within the input image, but is occluded.
            In this paper, we consider the sub-problem of face detection, since this class provides motivating applications involving safety and privacy, but do not tailor our method specifically to this class.
            Existing approaches rely on image sequences so that missing detections may be interpolated from surrounding frames or make use of generative models to sample possible completions.
            In contrast, we consider the single-image task and propose a more efficient, sample-free approach that makes use of the contextual cues from the image to infer the presence of unseen faces.
            We design a heatmap-based extreme amodal object detector that addresses the problem of efficiently predicting a lot (the out-of-frame region) from a little (the image) with a selective coarse-to-fine decoder.
            Our method establishes strong results for this new task, even outperforming less efficient generative approaches.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Task Definition Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Extreme Amodal Detection</h2>
        <div class="content has-text-justified">

          <p>
            Given an image \( \mathbf{x} \in \mathbb{R}^{H \times W \times 3} \), extreme amodal detection predicts the location of objects within a centrally-expanded region of size \( KH \times KW \), where \( K \) denotes the expansion factor.
            To predict objects within this larger region, we consider two output types, commonly associated with the tasks of detection and localization.
          </p>

          <p>
            For the <strong>detection task</strong>, a set of \( N \) objects \( o_i = (c_i, b_i) \) are predicted, where \( c_i \) is the object class and \( b_i = (x_i, y_i, w_i, h_i) \) is the bounding box represented by center coordinate, width, and height.
          </p>

          <p>
            For the <strong>localization task</strong>, a heatmap \( \mathbf{h} \in [0,1]^{KH \times KW \times C} \) is predicted, where \( C \) is the number of classes. The value at coordinate \( (i, j, c) \), denoted \( \hat{h}^c_{i,j} \), indicates the probability that an object of class \( c \) is located at that pixel.
          </p>

          <p>
            As motivated in the introduction, we consider a single class in this paper: <strong>human faces</strong>.
          </p>

          <!-- Task Image -->
          <figure class="image is-4by3">
            <img src="static/images/task.png" alt="Task definition illustration" loading="lazy">
            <figcaption class="has-text-centered is-size-7 mt-2">Illustration of the extreme amodal detection task.</figcaption>
          </figure>

          <p>
            As shown in the figure, the difficulty of detecting extreme amodal faces depends on whether there is direct visual evidence within the image of a face wholly or partially outside the image. We classify faces as follows:
          </p>

          <ol type="1">
            <li><strong>Inside:</strong> faces that are entirely within the image.</li>
            <li><strong>Truncated:</strong> faces that are partially within the image.</li>
            <li><strong>Outside:</strong> faces that are entirely outside the image:
              <ol type="a">
                <li><strong>With direct visual evidence</strong>, such as a visible body in the image.</li>
                <li><strong>Without direct visual evidence</strong>, where indirect cues like eye gaze and semantic co-occurrences may need to be considered.</li>
              </ol>
            </li>
          </ol>

        </div>
      </div>
    </div>
  </div>
</section>


<!-- Dataset Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The <code>EXAFace</code> Dataset</h2>
        <div class="content has-text-justified">

          <p>
            We introduce the <strong>Extreme Amodal Face (EXAFace)</strong> dataset, derived from the MS COCO object detection dataset.
            First, RetinaFace was used to pseudolabel the many unlabeled faces in COCO, excluding detections with a confidence below 0.9, resulting in <strong>2.4×</strong> more face labels.
            Then, we applied a randomized cropping strategy while preserving both cropped and uncropped bounding boxes.
          </p>

          <p>For an image with height \( H \) and width \( W \), the cropping procedure is:</p>
          <ol>
            <li>Sample crop height from \([0.3H, 0.6H]\) and aspect ratio from \([0.5, 2.0]\), producing crop size \( H' \times W' \).</li>
            <li>Sample crop center \( (x, y) \) from \([0.5W', W - 0.5W'] \times [0.5H', H - 0.5H']\).</li>
            <li>Crop the image based on the sampled center and size.</li>
            <li>Discard boxes not fully inside the expanded crop area (\( K^2 \times \) crop size).</li>
            <li>Update each box center \( (x_b, y_b) \rightarrow (x_b - x + 0.5KW', y_b - y + 0.5KH') \).</li>
          </ol>
          
          <p>This process is repeated four times per image to generate diverse examples. Image and box statistics are summarized below.</p>

          <!-- Dataset Table -->
          <div class="table-container mt-4">
            <table class="table is-bordered is-striped is-narrow is-hoverable is-size-7">
              <thead>
                <tr>
                  <th>♯ × 10³ (%)</th>
                  <th>Inside</th>
                  <th>Truncated</th>
                  <th>Outside +</th>
                  <th>Outside −</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Boxes (train)</strong></td>
                  <td>116 (24%)</td>
                  <td>74 (15%)</td>
                  <td>66 (13%)</td>
                  <td>235 (48%)</td>
                </tr>
                <tr>
                  <td><strong>Boxes (test)</strong></td>
                  <td>5.0 (24%)</td>
                  <td>3.0 (14%)</td>
                  <td>2.0 (12%)</td>
                  <td>11 (50%)</td>
                </tr>
                <tr>
                  <td><strong>Images (train)</strong></td>
                  <td>30 (17%)</td>
                  <td>37 (20%)</td>
                  <td>32 (17%)</td>
                  <td>83 (46%)</td>
                </tr>
                <tr>
                  <td><strong>Images (test)</strong></td>
                  <td>1.0 (16%)</td>
                  <td>1.5 (20%)</td>
                  <td>1.0 (17%)</td>
                  <td>3.5 (47%)</td>
                </tr>
              </tbody>
            </table>
          </div>

          <p class="is-size-7 mt-3">
            Sample counts (×10³) and percentages (in parentheses) are shown for bounding boxes and images.
            Categories include: <em>inside</em> faces, <em>truncated</em> faces, <em>outside</em> faces with body evidence (+), and <em>outside</em> faces without body evidence (−).
            An image is assigned a category based on the hardest face it contains.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
    
<!-- Image carousel -->
<!-- <section class="hero is-small"> -->
  <!-- <div class="hero-body"> -->
    <!-- <div class="container"> -->
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
       <!-- <div class="item"> -->
        <!-- TODO: Replace with your research result images -->
        <!-- <img src="static/images/carousel1.jpg" alt="First research result visualization" loading="lazy"/> -->
        <!-- TODO: Replace with description of this result -->
        <!-- <h2 class="subtitle has-text-centered"> -->
          <!-- First image description. -->
        <!-- </h2> -->
      <!-- </div> -->
      <!-- <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/carousel2.jpg" alt="Second research result visualization" loading="lazy"/> -->
        <!-- <h2 class="subtitle has-text-centered"> -->
          <!-- Second image description. -->
        <!-- </h2> -->
      <!-- </div> -->
      <!-- <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/carousel3.jpg" alt="Third research result visualization" loading="lazy"/> -->
        <!-- <h2 class="subtitle has-text-centered"> -->
         <!-- Third image description. -->
       <!-- </h2> -->
     <!-- </div> -->
     <!-- <div class="item"> -->
      <!-- Your image here -->
      <!-- <img src="static/images/carousel4.jpg" alt="Fourth research result visualization" loading="lazy"/> -->
      <!-- <h2 class="subtitle has-text-centered"> -->
        <!-- Fourth image description. -->
      <!-- </h2> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </div> -->
<!-- </div> -->
<!-- </section> -->
<!-- End image carousel -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Main Results</h2>
    <div class="table-container">
      <table class="table is-bordered is-striped is-fullwidth is-hoverable is-size-7">
        <thead>
          <tr>
            <th>Method</th>
            <th>\( \text{AP} \uparrow \)</th>
            <th class="has-background-light">\( \text{AP}_\text{t} \uparrow \)</th>
            <th>\( \text{AP}_\text{o} \uparrow \)</th>
            <th>\( \text{AP}_\text{o+} \uparrow \)</th>
            <th>\( \text{AP}_\text{o-} \uparrow \)</th>
            <th>\( \text{MAE} \downarrow \)</th>
            <th class="has-background-light">\( \text{MAE}_\text{t} \downarrow \)</th>
            <th>\( \text{MAE}_\text{o} \downarrow \)</th>
            <th>\( \text{MAE}_\text{o+} \downarrow \)</th>
            <th>\( \text{MAE}_\text{o-} \downarrow \)</th>
            <th class="has-background-light">\( \text{mIoU}_\text{o} \uparrow \)</th>
            <th>\( \text{AR}_\text{o} \uparrow \)</th>
            <th>\( \text{SE}_\text{o} \downarrow \)</th>
            <th>\( \text{CE}_\text{o} \downarrow \)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Uniform</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td>
            <td>--</td><td>--</td><td>--</td><td>--</td><td>--</td>
            <td>8.80</td><td>51.71</td><td>100</td><td>100</td>
          </tr>
          <tr>
            <td>Oracle-GT</td><td>100</td><td>100</td><td>100</td><td>100</td><td>100</td>
            <td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td>
            <td>100</td><td>100</td><td>58.68</td><td>58.68</td>
          </tr>
          <tr>
            <td>Oracle-YOLOH</td><td>44.79</td><td>61.70</td><td>36.34</td><td>49.83</td><td>22.85</td>
            <td>7.55</td><td>2.07</td><td>10.65</td><td>2.54</td><td>13.60</td>
            <td>28.63</td><td>44.56</td><td>91.96</td><td>78.74</td>
          </tr>
          <tr>
            <td>YOLOH</td>
            <td>10.20</td><td>30.60</td><td>0.01</td><td>0.01</td><td>0.001</td>
            <td><u>17.37</u></td><td>2.78</td><td>26.11</td><td>6.87</td><td><u>33.11</u></td>
            <td>17.23</td><td>19.01</td><td>96.90</td><td>94.01</td>
          </tr>
          <tr>
            <td>Pix2Gestalt</td>
            <td><u>11.30</u></td><td><u>33.43</u></td><td>0.24</td><td>0.48</td><td>0.001</td>
            <td>17.38</td><td>2.83</td><td><u>26.10</u></td><td>6.63</td><td>33.18</td>
            <td>17.75</td><td>20.25</td><td>96.54</td><td>93.31</td>
          </tr>
          <tr>
            <td>Outpaint</td>
            <td>4.93</td><td>11.54</td><td><b>1.62</b></td><td><b>2.47</b></td><td><b>0.76</b></td>
            <td><b>14.69</b></td><td>2.07</td><td><b>21.94</b></td><td><b>3.48</b></td><td><b>28.67</b></td>
            <td><b>20.53</b></td><td><u>25.03</u></td><td><u>96.41</u></td><td><u>90.18</u></td>
          </tr>
          <tr>
            <td><b>Ours</b></td>
            <td><b>23.07</b></td><td><b>66.69</b></td><td><u>1.26</u></td><td><u>2.17</u></td><td><u>0.34</u></td>
            <td>17.83</td><td><b>2.01</b></td><td>27.43</td><td><u>4.53</u></td><td>35.77</td>
            <td><u>18.70</u></td><td><b>27.17</b></td><td><b>93.99</b></td><td><b>88.16</b></td>
          </tr>
        </tbody>
      </table>
    </div>
    <p class="is-size-7 has-text-justified mt-4">
      Extreme amodal detection performance on the test set of our MS COCO-based dataset.
      We report metrics such as average precision (AP), mean absolute error (MAE), mean IoU, average recall (AR), self-entropy (SE), and cross-entropy (CE). Subscripts <i>t</i>, <i>o</i>, <i>o+</i>, and <i>o-</i> represent truncated, outside, outside with evidence, and outside without evidence, respectively. 
      The data subsets truncated (t), outside (o), outside with evidence (o+), and outside without evidence (o-) are indicated by subscripts.
      The metrics that are most meaningful for assessing performance on the different data subsets are shaded.
      Detection metrics like AP are appropriate for evaluation of the truncated faces, since the realization of the conditional distribution (our ``ground-truth'') is very close to the true distribution near the image.
      However, further from the image, this realization no longer captures all modes of the true distribution, and so AR, CE and SE are more meaningful measures of performance in this regime..
    </p>

    <!-- Qualitative result Image -->
          <figure class="image is-4by3">
            <img src="static/images/qual.png" alt="qualitative result" loading="lazy">
            <figcaption class="has-text-centered is-size-7 mt-2">Qualitative results.</figcaption>
          </figure>

          <p>
            The final row shows samples from the ground-truth conditional distributions. 
            Our model effectively leverages contextual cues—such as nearby people (example 1), objects like a skateboard (example 2), or partial body evidence (example 4)—to infer completely unseen faces. 
            In example 1, the model correctly extends predictions to the left, where a partial person is visible, but not to the right, demonstrating awareness of scene context and typical human height. 
            Example 3 further shows generalization beyond annotated ground truth. 
            Compared to our model, Pix2Gestalt struggles without large visible body parts, while the outpainting pipeline can infer outside faces but yields noisier and less consistent results.
          </p>

          <ol type="1">
            <li><strong>Inside:</strong> faces that are entirely within the image.</li>
            <li><strong>Truncated:</strong> faces that are partially within the image.</li>
            <li><strong>Outside:</strong> faces that are entirely outside the image:
              <ol type="a">
                <li><strong>With direct visual evidence</strong>, such as a visible body in the image.</li>
                <li><strong>Without direct visual evidence</strong>, where indirect cues like eye gaze and semantic co-occurrences may need to be considered.</li>
              </ol>
            </li>
          </ol>
  </div>
</section>




<!-- Youtube video -->
<!-- <section class="hero is-small is-light"> -->
  <!-- <div class="hero-body"> -->
    <!-- <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2> -->
      <!-- <div class="columns is-centered has-text-centered"> -->
        <!-- <div class="column is-four-fifths"> -->
          
          <!-- <div class="publication-video"> -->
            <!-- TODO: Replace with your YouTube video ID -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          <!-- </div> -->
        <!-- </div> -->
      <!-- </div> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small"> -->
  <!-- <div class="hero-body"> -->
    <!-- <div class="container"> -->
      <!-- <h2 class="title is-3">Another Carousel</h2> -->
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <!-- <div class="item item-video1"> -->
          <!-- TODO: Add poster image for better preview -->
          <!-- <video poster="" id="video1" controls muted loop height="100%" preload="metadata"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4" type="video/mp4"> -->
          <!-- </video> -->
        <!-- </div> -->
        <!-- <div class="item item-video2"> -->
          <!-- TODO: Add poster image for better preview -->
          <!-- <video poster="" id="video2" controls muted loop height="100%" preload="metadata"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4" type="video/mp4"> -->
          <!-- </video> -->
        <!-- </div> -->
        <!-- <div class="item item-video3"> -->
          <!-- TODO: Add poster image for better preview -->
          <!-- <video poster="" id="video3" controls muted loop height="100%" preload="metadata"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4" type="video/mp4"> -->
          <!-- </video> -->
        <!-- </div> -->
      <!-- </div> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light"> -->
  <!-- <div class="hero-body"> -->
    <!-- <div class="container"> -->
      <!-- <h2 class="title">Poster</h2> -->

      <!-- TODO: Replace with your poster PDF -->
      <!-- <iframe  src="static/pdfs/sample.pdf" width="100%" height="550"> -->
          <!-- </iframe> -->
        
      <!-- </div> -->
    <!-- </div> -->
  <!-- </section> -->
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@misc{song2025extremeamodalfacedetection,
      title={Extreme Amodal Face Detection}, 
      author={Changlin Song and Yunzhong Hou and Michael Randall Barnes and Rahul Shome and Dylan Campbell},
      year={2025},
      eprint={2510.06791},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2510.06791}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
